{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1556980134017_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-31-140.eu-central-1.compute.internal:20888/proxy/application_1556980134017_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-25-105.eu-central-1.compute.internal:8042/node/containerlogs/container_1556980134017_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Starter code\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, count,lit, when, col, desc, udf, col, sort_array, asc, avg, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdaf08da83b405795a999fd9def262a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist=u'Popol Vuh', auth=u'Logged In', firstName=u'Shlok', gender=u'M', itemInSession=278, lastName=u'Johnson', length=524.32934, level=u'paid', location=u'Dallas-Fort Worth-Arlington, TX', method=u'PUT', page=u'NextSong', registration=1533734541000, sessionId=22683, song=u'Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent=u'\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=u'1749042')"
     ]
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatize a cleaning and transforming process an ETL function was written, which originats from an investigation on \"mini\" version of the dataset. The function below does following:\n",
    "1. Cleans dataframe to remove any NaN from \"Gender\" columns (by removing userid '1261737' who is the only user without information about gender)\n",
    "2. Prepares timestamps and calculates time differences between user interactions with app\n",
    "3. Creates a userid aggregated table with 7 features defined in the study of \"mini\" dataset\n",
    "\n",
    "After application of the ETL function the resulting data is stored in .json file to avoid timeconsuming ETL in the future. As a result a \"df_feature\" table is read from a .json file to accelerate a ML Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512df9e59cf2450dac89810aa168cd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ETL(df):\n",
    "    '''\n",
    "    Function that performs ETL on a given dataframe\n",
    "    Input: df (pyspark Dataframe) - raw / source dataframe with user logs\n",
    "    Output: df_features (pyspark Dataframe) - resulting dataframe, aggregated by userid and containing 7 features and churn\n",
    "    \n",
    "    '''\n",
    "    # Clean dataset\n",
    "    df = df.filter(df.userId != '1261737')\n",
    "    \n",
    "    # Prepare Timestamps\n",
    "    ts_new = udf(lambda x: x / 1000)\n",
    "    df = df.withColumn('new_ts', ts_new('ts')).drop('ts')\n",
    "    df = df.withColumn('new_reg', ts_new('registration')).drop('registration')\n",
    "    \n",
    "    # Calculate Time Difference between user interactions with app\n",
    "    my_window = Window.partitionBy('userId').orderBy('new_ts')\n",
    "\n",
    "    df = df.withColumn('prev', lag(df.new_ts).over(my_window))\n",
    "    df = df.withColumn('diff_dates', F.when(F.isnull(df.new_ts - df.prev), 0)\n",
    "                      .otherwise(df.new_ts - df.prev))\n",
    "    \n",
    "    #Feature engineering\n",
    "    df_features = df.filter(df.page == 'NextSong') \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count','number_songs')\n",
    "\n",
    "    df_users_cancelled = df.filter(df.page == 'Cancellation Confirmation') \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count','Churn') \\\n",
    "        .withColumnRenamed('userId','user_canc')\n",
    "\n",
    "    udf_gender = udf(lambda x: 1 if x == 'M' else 0)\n",
    "    df_gender = df.dropDuplicates(['userId']) \\\n",
    "        .select(['userId','gender']) \\\n",
    "        .withColumnRenamed('userId','user_gender') \\\n",
    "        .withColumn('gender',udf_gender('gender'))\n",
    "        \n",
    "    df_gender = df_gender.withColumn('gender',df_gender.gender.cast(IntegerType()))\n",
    "    \n",
    "    df_thumbs_down = df.filter(df.page == 'Thumbs Down') \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count','Thumbs') \\\n",
    "        .withColumnRenamed('userId','user_thumbs')\n",
    "\n",
    "    df_reg = df.groupby('userId') \\\n",
    "        .agg({'new_ts':'max','new_reg':'max'}) \\\n",
    "        .withColumnRenamed('max(new_ts)','max_ts') \\\n",
    "        .withColumnRenamed('max(new_reg)','max_reg') \\\n",
    "        .withColumnRenamed('userId','userid_reg_len')\n",
    "\n",
    "    df_reg = df_reg.withColumn('reg_length',df_reg.max_ts - df_reg.max_reg) \\\n",
    "        .select('userid_reg_len','reg_length')\n",
    "\n",
    "    df_diff_dates_max = df.groupby('userId') \\\n",
    "        .agg({'diff_dates':'max'}) \\\n",
    "        .withColumnRenamed('max(diff_dates)','diff_dates_max') \\\n",
    "        .withColumnRenamed('userId','user_diff_dates_max')\n",
    "\n",
    "    df_diff_dates_mean = df.groupby('userId') \\\n",
    "        .agg({'diff_dates':'avg'}) \\\n",
    "        .withColumnRenamed('avg(diff_dates)','diff_dates_mean') \\\n",
    "        .withColumnRenamed('userId','user_diff_dates_mean')\n",
    "\n",
    "    df_diff_dates_week = df.filter(df.diff_dates > 600000) \\\n",
    "        .groupby('userId') \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed('count','diff_dates_session_week') \\\n",
    "        .withColumnRenamed('userid','user_diff_dates_week')\n",
    "\n",
    "    # Joints\n",
    "\n",
    "    df_features = df_features.join(df_users_cancelled, df_users_cancelled.user_canc == df_features.userId, how = 'left') \\\n",
    "        .drop(df_users_cancelled.user_canc)\n",
    "    df_features = df_features.join(df_gender, df_gender.user_gender == df_features.userId, how = 'left') \\\n",
    "        .drop(df_gender.user_gender)\n",
    "    df_features = df_features.join(df_thumbs_down, df_thumbs_down.user_thumbs == df_features.userId, how = 'left') \\\n",
    "        .drop(df_thumbs_down.user_thumbs)\n",
    "    df_features = df_features.join(df_reg, df_reg.userid_reg_len == df_features.userId, how = 'left') \\\n",
    "        .drop(df_reg.userid_reg_len)\n",
    "    df_features = df_features.join(df_diff_dates_max, df_diff_dates_max.user_diff_dates_max == df_features.userId, how = 'left') \\\n",
    "        .drop(df_diff_dates_max.user_diff_dates_max)\n",
    "    df_features = df_features.join(df_diff_dates_mean, df_diff_dates_mean.user_diff_dates_mean == df_features.userId, how = 'left') \\\n",
    "        .drop(df_diff_dates_mean.user_diff_dates_mean)\n",
    "    df_features = df_features.join(df_diff_dates_week, df_diff_dates_week.user_diff_dates_week == df_features.userId, how = 'left') \\\n",
    "        .drop(df_diff_dates_week.user_diff_dates_week)\n",
    "\n",
    "    df_features = df_features.fillna(0, subset=['Churn','Thumbs','diff_dates_session_week'])\n",
    "\n",
    "    df_features = df_features.withColumn('Thumbs_Down', F.expr('Thumbs / number_songs')).drop('Thumbs')\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4da0bd69a45492db186dc57e76725be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_features = ETL(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c9cd5e27574627b55b33c3edafcf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 42976)\n",
      "----------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python2.7/SocketServer.py\", line 655, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n",
      "    received_token = self.rfile.read(len(auth_token))\n",
      "TypeError: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "path_wr = 's3n://vk1009bucket1/Spark_try1/df_features.json'\n",
    "df_features.write.mode('append').json(path_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f75af3081b48638cfce3ce5ad0fa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_wr = 's3n://vk1009bucket1/Spark_try1/df_features.json'\n",
    "df_features = spark.read.json(path_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd68da764a1d47b28f772e0aa4ca16a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Churn=1, Thumbs_Down=0.016736401673640166, diff_dates_max=1686725.0, diff_dates_mean=6337.923076923077, diff_dates_session_week=1, gender=0, number_songs=239, reg_length=14986090.0, userId=u'1000353')"
     ]
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a4b268f5642a3944a49052cfe4f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import RegexTokenizer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Transformation / TrainTest Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ad0af0e0714727a210415932e5b0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define columns to use\n",
    "X = df_features.drop('userid')\n",
    "X_cols = X.schema.names\n",
    "X_cols.remove('Churn')\n",
    "\n",
    "# Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=X_cols, outputCol='features_vec')\n",
    "X = assembler.transform(X)\n",
    "\n",
    "# Standard Scaller\n",
    "scaler = StandardScaler(inputCol='features_vec', outputCol='sc_features',\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "ScalerModel = scaler.fit(X)\n",
    "\n",
    "X = ScalerModel.transform(X)\n",
    "\n",
    "# Train Test Split\n",
    "(trainingData, testData) = X.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Best ML Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2178903decfc43408e00bf74d30a05ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score achieved using Logistic Regression after Hyperparameter Tuning is 0.82"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"sc_features\")\n",
    "\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter,[5, 10, 20]) \\\n",
    "    .addGrid(lr.regParam,[0, 0.1, 1, 10]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "model_lr = crossval_lr.fit(trainingData)\n",
    "\n",
    "pred_lr = model_lr.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(pred_lr)\n",
    "print('The f1 score achieved using Logistic Regression after Hyperparameter Tuning is {}'.format(round(f1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c90e368a7b475286bbac3b724f831a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score achieved using Random Forest Classifier after Hyperparameter Tuning is 0.86"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\")\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees,[1, 10, 20]) \\\n",
    "    .addGrid(rf.maxDepth,[2, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid_rf,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "model_rf = crossval_rf.fit(trainingData)\n",
    "\n",
    "pred_rf = model_rf.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(pred_rf)\n",
    "print('The f1 score achieved using Random Forest Classifier after Hyperparameter Tuning is {}'.format(round(f1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score achieved using GBT Classifier after Hyperparameter Tuning is 0.86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/awseditorssparkmonitoringwidget-1.0-py3.6.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 7076\n",
      "\n",
      "Exception in thread cell_monitor-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/awseditorssparkmonitoringwidget-1.0-py3.6.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n",
      "    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\n",
      "KeyError: 7599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\")\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter,[5, 15]) \\\n",
    "    .addGrid(gbt.maxDepth,[2, 4]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "crossval_gbt = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid_gbt,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "model_gbt = crossval_gbt.fit(trainingData)\n",
    "\n",
    "pred_gbt = model_gbt.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(pred_gbt)\n",
    "print('The f1 score achieved using GBT Classifier after Hyperparameter Tuning is {}'.format(round(f1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose the best ML Classification alghorithm 3 approached were tested: Logistic Regression, Random Forest Classifier, GBT Classifier. Each alghorthm was tuned using hyperparameter tuning of at least two parameters.F1 Score was used as an evaluation metric.\n",
    "\n",
    "Result:\n",
    "1. Random Forest (F1 = 0.86)\n",
    "2. GBT Classifier (F1 = 0.86)\n",
    "3. Logistic Regression (F1 = 0.82)\n",
    "\n",
    "As expected ensemble methods showed a way better F1 Score in comparison with Logistic Regression. However, the difference between both ensemble alghorithms is tiny. So in the next part both alghorithms are investigated for feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature Importance from RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_rf = model_rf.bestModel\n",
    "\n",
    "best_num_trees = best_model_rf._java_obj.getNumTrees()\n",
    "best_max_depth = best_model_rf._java_obj.getMaxDepth()\n",
    "\n",
    "rf_best = RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", numTrees=best_num_trees, maxDepth=best_max_depth)\n",
    "\n",
    "model_fin = rf_best.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseVector(7, {0: 0.0836, 1: 0.2483, 2: 0.1167, 3: 0.1083, 4: 0.0129, 5: 0.177, 6: 0.2532})"
     ]
    }
   ],
   "source": [
    "model_fin.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thumbs_Down', 'diff_dates_max', 'diff_dates_mean', 'diff_dates_session_week', 'gender', 'number_songs', 'reg_length']"
     ]
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature Importance from GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4d5a8386d401e939bc9a2fca3ba3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model_gbt = model_gbt.bestModel\n",
    "\n",
    "best_max_iter = best_model_gbt._java_obj.getMaxIter()\n",
    "best_max_depth = best_model_gbt._java_obj.getMaxDepth()\n",
    "\n",
    "gbt_best = GBTClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", maxIter = best_max_iter, maxDepth=best_max_depth)\n",
    "\n",
    "model_fin_gbt = gbt_best.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f6a1114024b1cac549c7c0b06d6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseVector(7, {0: 0.1041, 1: 0.2062, 2: 0.2697, 3: 0.0375, 4: 0.0011, 5: 0.2676, 6: 0.1138})"
     ]
    }
   ],
   "source": [
    "model_fin_gbt.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b0be698c504348b691403f8cfae8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thumbs_Down', 'diff_dates_max', 'diff_dates_mean', 'diff_dates_session_week', 'gender', 'number_songs', 'reg_length']"
     ]
    }
   ],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to investigate the feature importnaces the best sets of parameters from both RF and GBT models were used for the final versions of the models. \n",
    "\n",
    "Result\n",
    "\n",
    "Random Forest Classifier's top features:\n",
    "1. 'reg_length' 0.253\n",
    "2. 'diff_dates_max' 0.248\n",
    "3. 'number_songs' 0.177\n",
    "\n",
    "GBT Classifier's top features:\n",
    "1. 'diff_dates_mean' 0.27\n",
    "2. 'number_songs' 0.268\n",
    "3. 'diff_dates_max' 0.206\n",
    "\n",
    "So, both classifiers agreed that the amount of songs user listened as well as the time difference between sessions are the most influential parameters for user to stay or cancel the subsription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
