{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "# Starter code\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import isnan, count,lit, when, col, desc, udf, col, sort_array, asc, avg, lag\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import sum as Fsum\n# Create spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "016efe73e89646f7b902367656d00c7e"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1557090982442_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-32.eu-central-1.compute.internal:20888/proxy/application_1557090982442_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-3-129.eu-central-1.compute.internal:8042/node/containerlogs/container_1557090982442_0001_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# Read in full sparkify dataset\nevent_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\ndf = spark.read.json(event_data)\ndf.head()", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cfdaf08da83b405795a999fd9def262a"}}, "metadata": {}}, {"output_type": "stream", "text": "Row(artist=u'Popol Vuh', auth=u'Logged In', firstName=u'Shlok', gender=u'M', itemInSession=278, lastName=u'Johnson', length=524.32934, level=u'paid', location=u'Dallas-Fort Worth-Arlington, TX', method=u'PUT', page=u'NextSong', registration=1533734541000, sessionId=22683, song=u'Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent=u'\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=u'1749042')", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "In order to automatize a cleaning and transforming process an ETL function was written, which originats from an investigation on \"mini\" version of the dataset. The function below does following:\n1. Cleans dataframe to remove any NaN from \"Gender\" columns (by removing userid '1261737' who is the only user without information about gender)\n2. Prepares timestamps and calculates time differences between user interactions with app\n3. Creates a userid aggregated table with 7 features defined in the study of \"mini\" dataset\n\nAfter application of the ETL function the resulting data is stored in .json file to avoid timeconsuming ETL in the future. As a result a \"df_feature\" table is read from a .json file to accelerate a ML Section."}, {"metadata": {}, "cell_type": "markdown", "source": "## ETL Pipeline"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def ETL(df):\n    '''\n    Function that implement an ETL Pipeline to the raw data\n    Input: df (pyspark dataframe) - raw data with user logs\n    Output: df_features (pyspark dataframe) - user aggregated dataframe\n    '''\n    # Clean dataset\n    df = df.filter(df.userId != '1261737')\n    \n    # Prepare Timestamps\n    ts_new = udf(lambda x: x / 1000)\n    df = df.withColumn('new_ts', ts_new('ts')).drop('ts')\n    df = df.withColumn('new_reg', ts_new('registration')).drop('registration')\n    \n    # Calculate Time Difference between user interactions with app\n    my_window = Window.partitionBy('userId').orderBy('new_ts')\n\n    df = df.withColumn('prev', lag(df.new_ts).over(my_window))\n    df = df.withColumn('diff_dates', F.when(F.isnull(df.new_ts - df.prev), 0)\n                      .otherwise(df.new_ts - df.prev))\n    \n    #Feature engineering\n    df_features = df.filter(df.page == 'NextSong') \\\n        .groupby('userId') \\\n        .count() \\\n        .withColumnRenamed('count','number_songs')\n\n    df_users_cancelled = df.filter(df.page == 'Cancellation Confirmation') \\\n        .groupby('userId') \\\n        .count() \\\n        .withColumnRenamed('count','Churn') \\\n        .withColumnRenamed('userId','user_canc')\n\n    udf_gender = udf(lambda x: 1 if x == 'M' else 0)\n    df_gender = df.dropDuplicates(['userId']) \\\n        .select(['userId','gender']) \\\n        .withColumnRenamed('userId','user_gender') \\\n        .withColumn('gender',udf_gender('gender'))\n        \n    df_gender = df_gender.withColumn('gender',df_gender.gender.cast(IntegerType()))\n    \n    df_thumbs_down = df.filter(df.page == 'Thumbs Down') \\\n        .groupby('userId') \\\n        .count() \\\n        .withColumnRenamed('count','Thumbs') \\\n        .withColumnRenamed('userId','user_thumbs')\n\n    df_reg = df.groupby('userId') \\\n        .agg({'new_ts':'max','new_reg':'max'}) \\\n        .withColumnRenamed('max(new_ts)','max_ts') \\\n        .withColumnRenamed('max(new_reg)','max_reg') \\\n        .withColumnRenamed('userId','userid_reg_len')\n\n    df_reg = df_reg.withColumn('reg_length',df_reg.max_ts - df_reg.max_reg) \\\n        .select('userid_reg_len','reg_length')\n\n    df_diff_dates_max = df.groupby('userId') \\\n        .agg({'diff_dates':'max'}) \\\n        .withColumnRenamed('max(diff_dates)','diff_dates_max') \\\n        .withColumnRenamed('userId','user_diff_dates_max')\n\n    df_diff_dates_mean = df.groupby('userId') \\\n        .agg({'diff_dates':'avg'}) \\\n        .withColumnRenamed('avg(diff_dates)','diff_dates_mean') \\\n        .withColumnRenamed('userId','user_diff_dates_mean')\n\n    df_diff_dates_week = df.filter(df.diff_dates > 600000) \\\n        .groupby('userId') \\\n        .count() \\\n        .withColumnRenamed('count','diff_dates_session_week') \\\n        .withColumnRenamed('userid','user_diff_dates_week')\n\n    # Joints\n\n    df_features = df_features.join(df_users_cancelled, df_users_cancelled.user_canc == df_features.userId, how = 'left') \\\n        .drop(df_users_cancelled.user_canc)\n    df_features = df_features.join(df_gender, df_gender.user_gender == df_features.userId, how = 'left') \\\n        .drop(df_gender.user_gender)\n    df_features = df_features.join(df_thumbs_down, df_thumbs_down.user_thumbs == df_features.userId, how = 'left') \\\n        .drop(df_thumbs_down.user_thumbs)\n    df_features = df_features.join(df_reg, df_reg.userid_reg_len == df_features.userId, how = 'left') \\\n        .drop(df_reg.userid_reg_len)\n    df_features = df_features.join(df_diff_dates_max, df_diff_dates_max.user_diff_dates_max == df_features.userId, how = 'left') \\\n        .drop(df_diff_dates_max.user_diff_dates_max)\n    df_features = df_features.join(df_diff_dates_mean, df_diff_dates_mean.user_diff_dates_mean == df_features.userId, how = 'left') \\\n        .drop(df_diff_dates_mean.user_diff_dates_mean)\n    df_features = df_features.join(df_diff_dates_week, df_diff_dates_week.user_diff_dates_week == df_features.userId, how = 'left') \\\n        .drop(df_diff_dates_week.user_diff_dates_week)\n\n    df_features = df_features.fillna(0, subset=['Churn','Thumbs','diff_dates_session_week'])\n\n    df_features = df_features.withColumn('Thumbs_Down', F.expr('Thumbs / number_songs')).drop('Thumbs')\n    \n    return df_features", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "512df9e59cf2450dac89810aa168cd45"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_features = ETL(df)", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a4da0bd69a45492db186dc57e76725be"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "path_wr = 's3n://vk1009bucket1/Spark_try1/df_features.json'\ndf_features.write.mode('append').json(path_wr)", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "35c9cd5e27574627b55b33c3edafcf70"}}, "metadata": {}}, {"output_type": "stream", "text": "----------------------------------------\nException happened during processing of request from ('127.0.0.1', 42976)\n----------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 321, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 334, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 655, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 266, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 241, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 254, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "path_wr = 's3n://vk1009bucket1/Spark_try1/df_features.json'\ndf_features = spark.read.json(path_wr)", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f40fe0ffc7904899a8afa6e0dc3df5b2"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_features.head()", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3638cd411d8e45688596275ee8535588"}}, "metadata": {}}, {"output_type": "stream", "text": "Row(Churn=1, Thumbs_Down=0.016736401673640166, diff_dates_max=1686725.0, diff_dates_mean=6337.923076923077, diff_dates_session_week=1, gender=0, number_songs=239, reg_length=14986090.0, userId=u'1000353')", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "### Prepare for ML"}, {"metadata": {}, "cell_type": "markdown", "source": "##### Load Libraries"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import RegexTokenizer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport numpy as np", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "00d44a451d204465bc3efed9aba7b870"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### Data Transformation / TrainTest Split"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Define columns to use\nX = df_features.drop('userid')\nX_cols = X.schema.names\nX_cols.remove('Churn')\n\n# Vector Assembler\nassembler = VectorAssembler(inputCols=X_cols, outputCol='features_vec')\nX = assembler.transform(X)\n\n# Standard Scaller\nscaler = StandardScaler(inputCol='features_vec', outputCol='sc_features',\n                        withStd=True, withMean=False)\n\nScalerModel = scaler.fit(X)\n\nX = ScalerModel.transform(X)\n\n# Train Test Split\n(trainingData, testData) = X.randomSplit([0.7, 0.3])", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e578fab3e60942d8801396b31cac17e7"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Search for Best ML Classifier"}, {"metadata": {}, "cell_type": "markdown", "source": "In order to find the best classifier 3 classifiers available in spark.ml library were tested: Logistic Regression, Random Forest Classifier and Gradient Boost Classifier.\n\nUsing Parameter Grid the hyperparameters of all above mentioned classifiers were tuned to achieve the best model. \n\n**Logistic Regression**: \n- maxIter: maximum number of iterations to run the optimization algorithm. Here I used the values of 5,10 and 20.\n- regParam: regularisation term. Here I used the values 0,0.1,1,10. For regularisation parameters the 10 fold difference between neighbor values is a common choice.\n\n**Random Forest** \n- numTrees: number of trees in the Random Forest classifier. I used values of 1,10 and 20. 1 corresponds to a Decision Tree. I didn't choose the value >20 in order to not slow down the program.\n- maxDepth: maximum depth of a single tree in the forest. I used 2,5,10. The values were chosen based on the size of the features: 7. \n\n**GBT Classifier** \n- maxIter: maximum number of iterations to run the optimization algorithm. Here I used the values of 5 and 15.\n- maxDepth: maximum depth of a single tree in the model. I used 2 and 4. The values were chosen based on the size of the features: 7. \n\nTo evaluate the model F1 score was used. F1 represents a balance between precision and recall. Here I can not see whether there is a certain preference for either recall or precision. On the one hand we want to identify as many uncertain customers as possible (recall high) but at the same time we do not want to offer too many discounts (precision low). So the balance between precision and recall is important here.  \nF1 score also works if the data is unbalanced, as we have here. Accuracy is therefore not the best choice."}, {"metadata": {}, "cell_type": "markdown", "source": "##### Logistic Regression"}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "# Initialize a model\nlr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"sc_features\")\n\n# Define Parameter Grid\nparamGrid_lr = ParamGridBuilder() \\\n    .addGrid(lr.maxIter,[5, 10, 20]) \\\n    .addGrid(lr.regParam,[0, 0.1, 1, 10]) \\\n    .build()\n\n# Define CV \ncrossval_lr = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid_lr,\n                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n                          numFolds=3)\n# Fit & predict\nmodel_lr = crossval_lr.fit(trainingData)\n\npred_lr = model_lr.transform(testData)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluator.evaluate(pred_lr)\nprint('The f1 score achieved using Logistic Regression after Hyperparameter Tuning is {}'.format(round(f1,2)))", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "32632960792544fbb09a6cebd95cb902"}}, "metadata": {}}, {"output_type": "stream", "text": "The f1 score achieved using Logistic Regression after Hyperparameter Tuning is 0.82", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "What are the best parameters for LR Classifier"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "best_model_lr = model_lr.bestModel\n\nbest_max_iter = best_model_lr._java_obj.getMaxIter()\nbest_reg_param = best_model_lr._java_obj.getRegParam()\nprint('Optimal parameters for LR Classifier: maxIter {}, regParam {}'.format(best_max_iter, best_reg_param))", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "914bec8342ba43e4827bfafbe8c54e2c"}}, "metadata": {}}, {"output_type": "stream", "text": "Optimal parameters for LR Classifier: maxIter 10, regParam 0.0", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### Random Forest"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Initialize a model\nrf = RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\")\n\n# Define Parameter Grid\nparamGrid_rf = ParamGridBuilder() \\\n    .addGrid(rf.numTrees,[1, 10, 20]) \\\n    .addGrid(rf.maxDepth,[2, 5, 10]) \\\n    .build()\n\n# Define CV\ncrossval_rf = CrossValidator(estimator=rf,\n                          estimatorParamMaps=paramGrid_rf,\n                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n                          numFolds=3)\n# Fit & predict\nmodel_rf = crossval_rf.fit(trainingData)\n\npred_rf = model_rf.transform(testData)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluator.evaluate(pred_rf)\nprint('The f1 score achieved using Random Forest Classifier after Hyperparameter Tuning is {}'.format(round(f1,2)))", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2502c7141fa94f25b5bc21030bd13b2c"}}, "metadata": {}}, {"output_type": "stream", "text": "The f1 score achieved using Random Forest Classifier after Hyperparameter Tuning is 0.86", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "What are the best tuning Parameters for RF Classifier:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "best_model_rf = model_rf.bestModel\n\nbest_num_trees = best_model_rf._java_obj.getNumTrees()\nbest_max_depth = best_model_rf._java_obj.getMaxDepth()\nprint('Optimal parameters for RF Classifier: numTrees {}, maxDepth {}'.format(best_num_trees, best_max_depth))", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "eab11ea853f344dcbc784e73b51ef7d3"}}, "metadata": {}}, {"output_type": "stream", "text": "Optimal parameters for RF Classifier: numTrees 20, maxDepth 10", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### GBT Classifier"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Initialize a model\ngbt = GBTClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\")\n\n# Define Parameter Grid\nparamGrid_gbt = ParamGridBuilder() \\\n    .addGrid(gbt.maxIter,[5, 15]) \\\n    .addGrid(gbt.maxDepth,[2, 4]) \\\n    .build()\n\n# Define CV\ncrossval_gbt = CrossValidator(estimator=gbt,\n                          estimatorParamMaps=paramGrid_gbt,\n                          evaluator=MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\"),\n                          numFolds=3)\n# Fit & predict\nmodel_gbt = crossval_gbt.fit(trainingData)\n\npred_gbt = model_gbt.transform(testData)\n\n#Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluator.evaluate(pred_gbt)\nprint('The f1 score achieved using GBT Classifier after Hyperparameter Tuning is {}'.format(round(f1,2)))", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "75d646dd76ab480d84383a108a642aae"}}, "metadata": {}}, {"output_type": "stream", "text": "Exception in thread cell_monitor-7:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/awseditorssparkmonitoringwidget-1.0-py3.6.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 178, in cell_monitor\n    job_binned_stages[job_id][stage_id] = all_stages[stage_id]\nKeyError: 1958\n\n", "name": "stderr"}, {"output_type": "stream", "text": "The f1 score achieved using GBT Classifier after Hyperparameter Tuning is 0.86", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "What are the best parameters for GBT Classifier"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "best_model_gbt = model_gbt.bestModel\n    \n# get the best parameters for a model\nbest_max_iter = best_model_gbt._java_obj.getMaxIter()\nbest_max_depth = best_model_gbt._java_obj.getMaxDepth()\n\nprint('Optimal parameters for GBT Classifier: maxIter {}, maxDepth {}'.format(best_max_iter, best_max_depth))", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "74b366145503417e97ed6ab91f466794"}}, "metadata": {}}, {"output_type": "stream", "text": "Optimal parameters for GBT Classifier: maxIter 15, maxDepth 4", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### Conclusion"}, {"metadata": {}, "cell_type": "markdown", "source": "In order to choose the best ML Classification alghorithm 3 approached were tested: Logistic Regression, Random Forest Classifier, GBT Classifier. Each alghorthm was tuned using hyperparameter tuning of at least two parameters.F1 Score was used as an evaluation metric.\n\nResult:\n1. Random Forest (F1 = 0.86)\n2. GBT Classifier (F1 = 0.86)\n3. Logistic Regression (F1 = 0.82)\n\nAs expected ensemble methods showed a way better F1 Score in comparison with Logistic Regression. However, the difference between both ensemble alghorithms is tiny. So in the next part both alghorithms are investigated for feature importance."}, {"metadata": {}, "cell_type": "markdown", "source": "### Model Robustness investigation"}, {"metadata": {}, "cell_type": "markdown", "source": "##### Random Forest Classifier"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section the robustness of the best RF Model is investigated. To test it the train/test data was shuffeled 10 times and each time the best RF Model was applied to the data. As a result we get 10 f1 scores for train and test data. "}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "# Initialise lists for stat calculation\nf1_list_train = []\nf1_list_test = []\n\n# make 10 simulations\nfor i in range(10):\n    \n    # random shuffle of train/test data\n    (trainingData, testData) = X.randomSplit([0.7, 0.3])\n    \n    # recall best RF Model from CV Model implemented above\n    best_model_rf = model_rf.bestModel\n    \n    # get the best parameters for a model\n    best_num_trees = best_model_rf._java_obj.getNumTrees()\n    best_max_depth = best_model_rf._java_obj.getMaxDepth()\n    \n    # Initialize a RF Model with the best parameters set\n    rf_best = RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", numTrees=best_num_trees, maxDepth=best_max_depth)\n    \n    # Train a model\n    model_fin = rf_best.fit(trainingData)\n    \n    # Initialize an evaluator\n    evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n\n    # Calculate f1 for train dataset\n    pred_rf_train = model_fin.transform(trainingData)\n    f1_train = evaluator.evaluate(pred_rf_train)\n    \n    # Calculate f1 for test dataset\n    pred_rf_test = model_fin.transform(testData)\n    f1_test = evaluator.evaluate(pred_rf_test)\n    \n    # Append the f1 scores to corresponding lists\n    f1_list_train.append(f1_train)\n    f1_list_test.append(f1_test)\n    \n    print('Dataset Split {}: Train f1 score {}, Test f1 score {}'.format(i+1,round(f1_train,5), round(f1_test,5)))", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "Dataset Split 1: Train f1 score 0.8991, Test f1 score 0.86112\nDataset Split 2: Train f1 score 0.89564, Test f1 score 0.8654\nDataset Split 3: Train f1 score 0.89961, Test f1 score 0.85693\nDataset Split 4: Train f1 score 0.89709, Test f1 score 0.8611\nDataset Split 5: Train f1 score 0.90084, Test f1 score 0.85926\nDataset Split 6: Train f1 score 0.89829, Test f1 score 0.8633\nDataset Split 7: Train f1 score 0.90139, Test f1 score 0.86606\nDataset Split 8: Train f1 score 0.89626, Test f1 score 0.86567\nDataset Split 9: Train f1 score 0.8971, Test f1 score 0.8597\nDataset Split 10: Train f1 score 0.89775, Test f1 score 0.8659", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Calculate some statistics"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_mean = round(np.mean(f1_list_train),3)\ntest_mean = round(np.mean(f1_list_test),3)\n\ntrain_std = round(np.std(f1_list_train),3)\ntest_std = round(np.std(f1_list_test),3)\n\nprint('Results after 10 simulations: Train Mean {}, Train Std {}; Test Mean {}, Test Std {}'.format(train_mean, train_std, \n                                                                                                    test_mean, test_std))", "execution_count": 26, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4cffda92f69e4b6ea12352a9e2fbef79"}}, "metadata": {}}, {"output_type": "stream", "text": "Results after 10 simulations: Train Mean 0.898, Train Std 0.002; Test Mean 0.862, Test Std 0.003", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "As we can see both Train and Test F1 scores remain very stable over 10 folds. Thus the Model is robust. The differnce between Train and Tests scores of 0.036 in my point of view is acceptable. Therefore I can conclude that this model is not overfitted."}, {"metadata": {}, "cell_type": "markdown", "source": "##### GBT Classifier"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section the robustness of the best GBT Model is investigated. To test it the train/test data was shuffeled 10 times and each time the best GBT Model was applied to the data. As a result we get 10 f1 scores for train and test data. "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Initialise lists for stat calculation\nf1_list_train = []\nf1_list_test = []\n\n# make 10 simulations\nfor i in range(10):\n    \n    # random shuffle of train/test data\n    (trainingData, testData) = X.randomSplit([0.7, 0.3])\n    \n    # recall best RF Model from CV Model implemented above\n    best_model_gbt = model_gbt.bestModel\n    \n    # get the best parameters for a model\n    best_max_iter = best_model_gbt._java_obj.getMaxIter()\n    best_max_depth = best_model_gbt._java_obj.getMaxDepth()\n\n    # Initialize a RF Model with the best parameters set\n    gbt_best = GBTClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", maxIter = best_max_iter, maxDepth=best_max_depth)\n    \n    # Train a model\n    model_fin_gbt = gbt_best.fit(trainingData)\n    \n    # Initialize an evaluator\n    evaluator = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\", metricName=\"f1\")\n\n    # Calculate f1 for train dataset\n    pred_rf_train = model_fin_gbt.transform(trainingData)\n    f1_train = evaluator.evaluate(pred_rf_train)\n    \n    # Calculate f1 for test dataset\n    pred_rf_test = model_fin_gbt.transform(testData)\n    f1_test = evaluator.evaluate(pred_rf_test)\n    \n    # Append the f1 scores to corresponding lists\n    f1_list_train.append(f1_train)\n    f1_list_test.append(f1_test)\n    \n    print('Dataset Split {}: Train f1 score {}, Test f1 score {}'.format(i+1,round(f1_train,5), round(f1_test,5)))", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fd735c02ef964d1991a216d09c93ab0b"}}, "metadata": {}}, {"output_type": "stream", "text": "Dataset Split 1: Train f1 score 0.85686, Test f1 score 0.85263\nDataset Split 2: Train f1 score 0.86212, Test f1 score 0.85002\nDataset Split 3: Train f1 score 0.85956, Test f1 score 0.8535\nDataset Split 4: Train f1 score 0.86038, Test f1 score 0.86126\nDataset Split 5: Train f1 score 0.85902, Test f1 score 0.85804\nDataset Split 6: Train f1 score 0.85939, Test f1 score 0.85436\nDataset Split 7: Train f1 score 0.86245, Test f1 score 0.85262\nDataset Split 8: Train f1 score 0.85329, Test f1 score 0.84343\nDataset Split 9: Train f1 score 0.86521, Test f1 score 0.852\nDataset Split 10: Train f1 score 0.85876, Test f1 score 0.84737", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Calculate some statistics"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "train_mean = round(np.mean(f1_list_train),3)\ntest_mean = round(np.mean(f1_list_test),3)\n\ntrain_std = round(np.std(f1_list_train),3)\ntest_std = round(np.std(f1_list_test),3)\n\nprint('Results after 10 simulations: Train Mean {}, Train Std {}; Test Mean {}, Test Std {}'.format(train_mean, train_std, \n                                                                                                    test_mean, test_std))", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "72b41d4c797040b79da136c4eb4a9e04"}}, "metadata": {}}, {"output_type": "stream", "text": "Results after 10 simulations: Train Mean 0.86, Train Std 0.003; Test Mean 0.853, Test Std 0.005", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "As we can see both Train and Test F1 scores remain very stable over 10 folds. Thus the Model is robust. The differnce between Train and Tests scores of 0.007 is tiny. Therefore I can conclude that this model is not overfitted."}, {"metadata": {}, "cell_type": "markdown", "source": "### Get feature Importance from RF Classifier"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# define best model\nbest_model_rf = model_rf.bestModel\n\nbest_num_trees = best_model_rf._java_obj.getNumTrees()\nbest_max_depth = best_model_rf._java_obj.getMaxDepth()\n\nrf_best = RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", numTrees=best_num_trees, maxDepth=best_max_depth)\n\n# fit best model\nmodel_fin = rf_best.fit(trainingData)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_fin.featureImportances", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "SparseVector(7, {0: 0.0836, 1: 0.2483, 2: 0.1167, 3: 0.1083, 4: 0.0129, 5: 0.177, 6: 0.2532})", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "X_cols", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "stream", "text": "['Thumbs_Down', 'diff_dates_max', 'diff_dates_mean', 'diff_dates_session_week', 'gender', 'number_songs', 'reg_length']", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Get feature Importance from GBT Classifier"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# define best model\nbest_model_gbt = model_gbt.bestModel\n\nbest_max_iter = best_model_gbt._java_obj.getMaxIter()\nbest_max_depth = best_model_gbt._java_obj.getMaxDepth()\n\ngbt_best = GBTClassifier(labelCol=\"Churn\", featuresCol=\"sc_features\", maxIter = best_max_iter, maxDepth=best_max_depth)\n\n# fit best model\nmodel_fin_gbt = gbt_best.fit(trainingData)", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "98a4d5a8386d401e939bc9a2fca3ba3e"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_fin_gbt.featureImportances", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "349f6a1114024b1cac549c7c0b06d6df"}}, "metadata": {}}, {"output_type": "stream", "text": "SparseVector(7, {0: 0.1041, 1: 0.2062, 2: 0.2697, 3: 0.0375, 4: 0.0011, 5: 0.2676, 6: 0.1138})", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "X_cols", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a2b0be698c504348b691403f8cfae8fe"}}, "metadata": {}}, {"output_type": "stream", "text": "['Thumbs_Down', 'diff_dates_max', 'diff_dates_mean', 'diff_dates_session_week', 'gender', 'number_songs', 'reg_length']", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### Conclusion"}, {"metadata": {}, "cell_type": "markdown", "source": "In order to investigate the feature importnaces the best sets of parameters from both RF and GBT models were used for the final versions of the models. \n\nResult\n\nRandom Forest Classifier's top features:\n1. 'reg_length' 0.253\n2. 'diff_dates_max' 0.248\n3. 'number_songs' 0.177\n\nGBT Classifier's top features:\n1. 'diff_dates_mean' 0.27\n2. 'number_songs' 0.268\n3. 'diff_dates_max' 0.206\n\nSo, both classifiers agreed that the amount of songs user listened as well as the time difference between sessions are the most influential parameters for user to stay or cancel the subsription."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}